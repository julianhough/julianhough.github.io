@article{SumanathilakaEtAl25EDI40,
title = {GlossGPT: GPT for Word Sense Disambiguation using Few-shot Chain-of-Thought Prompting},
journal = {Procedia Computer Science},
volume = {257},
pages = {785-792},
year = {2025},
note = {The 16th International Conference on Ambient Systems, Networks and Technologies Networks (ANT)/ the 8th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925008385},
author = {Deshan Sumanathilaka and Nicholas Micallef and Julian Hough},
keywords = {Word Sense Disambiguation, Knowledge Base Retrieval, Large Language Models, GPT-4-Turbo, Chain of Thought Prompting},
abstract = {Lexical ambiguity is a major challenge in computational linguistic tasks, as limitations in proper sense identification lead to inefficient translation and question answering. General-purpose Large Language Models (LLMs) are commonly utilized for Natural Language Processing (NLP) tasks. However, utilizing general-purpose LLMs for specific tasks has been challenging, and fine-tuning has become a critical requirement for task specification. In this work, we craft advanced prompts with different contextual parameters to guide the modelâ€™s inference towards accurate sense prediction to handle Word Sense Disambiguation (WSD). We present a few-shot Chain of Thought (COT) prompt-based technique using GPT-4-Turbo with knowledgebase as a retriever that does not require fine-tuning the model for WSD tasks and sense definitions are supported by synonyms to broaden the lexical meaning. Our approach achieves comparable performance on the SemEval and Senseval datasets. More importantly, we set a new state-of-the-art performance with the few-shot FEWS dataset, breaking through the 90% F1 score barrier.}
}